{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a912e99-9057-4d21-8c57-9e2bd111e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector-assembly_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8d689389-966d-4cab-9682-4feab449c611;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-assembly_2.12;3.1.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-assembly_2.12/3.1.0/spark-cassandra-connector-assembly_2.12-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-assembly_2.12;3.1.0!spark-cassandra-connector-assembly_2.12.jar (2235ms)\n",
      ":: resolution report :: resolve 1764ms :: artifacts dl 2238ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.spark#spark-cassandra-connector-assembly_2.12;3.1.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8d689389-966d-4cab-9682-4feab449c611\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 0 already retrieved (14544kB/26ms)\n",
      "23/06/11 17:55:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# CASSANDRA CONFIGURATION\n",
    "cassandra_host = \"cassandra\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "      .config(\"spark.cassandra.connection.host\", cassandra_host) \\\n",
    "      .config(\"spark.jars.packages\",\"com.datastax.spark:spark-cassandra-connector-assembly_2.12:3.1.0\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb174b8-bb75-4e7e-abc5-cbab5326e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q cassandra-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83603b27-6279-4f9f-92c5-5052c70d1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://archive-api.open-meteo.com/v1/archive?latitude=43.0481&longitude=76.1474&start_date=2023-05-16&end_date=2023-05-30&hourly=temperature_2m,dewpoint_2m,precipitation,rain,snowfall,weathercode,cloudcover,cloudcover_low,windspeed_10m,windspeed_100m,winddirection_10m,winddirection_100m,windgusts_10m&temperature_unit=fahrenheit')\n",
    "#response = requests.get('https://api.open-meteo.com/v1/forecast?latitude=43.05&start_date=2023-05-16&end_date=2023-05-30&longitude=-76.15&daily=weathercode,temperature_2m_max,temperature_2m_min,sunrise,sunset,precipitation_sum,rain_sum,showers_sum,snowfall_sum,precipitation_hours,precipitation_probability_max,windspeed_10m_max,windgusts_10m_max&temperature_unit=fahrenheit&timezone=auto')\n",
    "data = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0caaf1ae-a273-4e0f-bc81-efdcc418cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,explode\n",
    "import pandas as pd\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "#save data to json file and csv\n",
    "df.to_json(\"project_weather_data_meteo.json2\", orient=\"records\")\n",
    "df.to_csv(\"project_weather_data_meteo.csv2\",index=False,header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f036232-0cef-44d7-8540-ccd595b28d53",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'daily'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_56/1097565967.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Extract the hourly data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhourly_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'daily'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Get the list of available variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'daily'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#try to rename the column from the json\n",
    "#hourly_data = data['daily']\n",
    "\n",
    "#variables = hourly_data.keys()\n",
    "\n",
    "# Write the flattened data to a CSV file\n",
    "#with open('project_weather_data_meteo.csv2', 'w', newline='') as file:\n",
    "   # writer = csv.writer(file)\n",
    "\n",
    "    # Write the header row with variable names\n",
    "    writer.writerow(['Date', 'Time'] + list(variables))\n",
    "\n",
    "    # Iterate over each hour\n",
    "   # for i in range(len(hourly_data['time'])):\n",
    "        row = []\n",
    "\n",
    "        # Extract the date and time\n",
    "    #    date, time = hourly_data['time'][i].split('T')\n",
    "     #   row.extend([date, time])\n",
    "\n",
    "        # Extract the values for each variable\n",
    "     #   for variable in variables:\n",
    "      #      value = hourly_data[variable][i]\n",
    "       #     row.append(value)\n",
    "\n",
    "     #   writer.writerow(row)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4eaa14d-3d19-41e6-bb89-816260d1908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- generationtime_ms: double (nullable = true)\n",
      " |-- hourly: struct (nullable = true)\n",
      " |    |-- cloudcover: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- cloudcover_low: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- dewpoint_2m: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- precipitation: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- rain: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- snowfall: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- temperature_2m: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- time: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- weathercode: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- winddirection_100m: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- winddirection_10m: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- windgusts_10m: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- windspeed_100m: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- windspeed_10m: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- hourly_units: struct (nullable = true)\n",
      " |    |-- cloudcover: string (nullable = true)\n",
      " |    |-- cloudcover_low: string (nullable = true)\n",
      " |    |-- dewpoint_2m: string (nullable = true)\n",
      " |    |-- precipitation: string (nullable = true)\n",
      " |    |-- rain: string (nullable = true)\n",
      " |    |-- snowfall: string (nullable = true)\n",
      " |    |-- temperature_2m: string (nullable = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |    |-- weathercode: string (nullable = true)\n",
      " |    |-- winddirection_100m: string (nullable = true)\n",
      " |    |-- winddirection_10m: string (nullable = true)\n",
      " |    |-- windgusts_10m: string (nullable = true)\n",
      " |    |-- windspeed_100m: string (nullable = true)\n",
      " |    |-- windspeed_10m: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- timezone_abbreviation: string (nullable = true)\n",
      " |-- utc_offset_seconds: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#open csv \n",
    "file_path = 'file:///home/jovyan/datasets/project_weather_data_meteo.json'\n",
    "weather = spark.read.json(file_path)\n",
    "weather.printSchema()\n",
    "#save as dataframe and see the schema \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db96dfa-c585-479b-90da-6b7a3122409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weathertest = weather.select(weather.hourly.time)\n",
    "#weathertest = weather.select(weather.hourly_units.time)\n",
    "#df = weather.select(explode(\"hourly_units.time\").alias(\"time\")).select(\"time.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c43fa6-4f4b-4cea-9fb5-4e11646b7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_weather =  \"/home/jovyan/datasets/project_weather_data_meteo.csv\"\n",
    "\n",
    "df_weather= pd.read_csv(file_weather)\n",
    "df_weather = df_weather.rename(columns={'Time': 'hour'})\n",
    "\n",
    "df_spark = spark.createDataFrame(df_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "308d5ca6-b6ea-444b-b4f2-a68651aaf760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- temperature_2m: double (nullable = true)\n",
      " |-- dewpoint_2m: double (nullable = true)\n",
      " |-- precipitation: double (nullable = true)\n",
      " |-- rain: double (nullable = true)\n",
      " |-- snowfall: double (nullable = true)\n",
      " |-- weathercode: long (nullable = true)\n",
      " |-- cloudcover: long (nullable = true)\n",
      " |-- cloudcover_low: long (nullable = true)\n",
      " |-- windspeed_10m: double (nullable = true)\n",
      " |-- windspeed_100m: double (nullable = true)\n",
      " |-- winddirection_10m: long (nullable = true)\n",
      " |-- winddirection_100m: long (nullable = true)\n",
      " |-- windgusts_10m: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67c58967-3539-4c21-863d-39825f8cf807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE IF NOT EXISTS project_weather_data.weather_data_table (\n",
      "    date date, \n",
      "    hour time,\n",
      "    time timestamp,\n",
      "    temperature_2m float,\n",
      "    dewpoint_2m float,\n",
      "    precipitation float,\n",
      "    rain float,\n",
      "    snowfall float,\n",
      "    weathercode float,\n",
      "    cloudcover float,\n",
      "    cloudcover_low float,\n",
      "    windspeed_10m float,\n",
      "    windspeed_100m float,\n",
      "    winddirection_10m float,\n",
      "    winddirection_100m float,\n",
      "    windgusts_10m float,\n",
      "    PRIMARY KEY (time)\n",
      "\n",
      "\n",
      "\n",
      "  );\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the data to Cassandra\n",
    "create_table_sql =  '''\n",
    "CREATE TABLE IF NOT EXISTS project_weather_data.weather_data_table (\n",
    "    date date, \n",
    "    hour time,\n",
    "    time timestamp,\n",
    "    temperature_2m float,\n",
    "    dewpoint_2m float,\n",
    "    precipitation float,\n",
    "    rain float,\n",
    "    snowfall float,\n",
    "    weathercode float,\n",
    "    cloudcover float,\n",
    "    cloudcover_low float,\n",
    "    windspeed_10m float,\n",
    "    windspeed_100m float,\n",
    "    winddirection_10m float,\n",
    "    winddirection_100m float,\n",
    "    windgusts_10m float,\n",
    "    PRIMARY KEY (time)\n",
    "\n",
    "\n",
    "\n",
    "  );\n",
    "  '''\n",
    "from cassandra.cluster import Cluster\n",
    "with Cluster([cassandra_host]) as cluster:\n",
    "    session = cluster.connect()\n",
    "    session.execute(create_table_sql)\n",
    "print(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88c3dc1-dc5b-4879-a65c-fd9c21fbc217",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2 = df_spark.toDF(\"date\",\n",
    "\"hour\",\n",
    "\"time\",\n",
    "\"temperature_2m\",\n",
    "\"dewpoint_2m\",\n",
    "\"precipitation\",\n",
    "\"rain\",\n",
    "\"snowfall\",\n",
    "\"weathercode\",\n",
    "\"cloudcover\",\n",
    "\"cloudcover_low\",\n",
    "\"windspeed_10m\",\n",
    "\"windspeed_100m\",\n",
    "\"winddirection_10m\",\n",
    "\"winddirection_100m\",\n",
    "\"windgusts_10m\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc9ea51-bb92-41de-bdd8-fd5d62ce2340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather_2.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"table\", \"weather_data_table\")\\\n",
    "    .option(\"keyspace\", \"project_weather_data\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c9555c-ca63-435f-984a-3f9b41b83625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_3 =spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .option(\"table\", \"weather_data_table\")\\\n",
    "    .option(\"keyspace\",\"project_weather_data\")\\\n",
    "    .load()\n",
    "\n",
    "weather_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fadb90-098b-48d7-9cb7-fbe59fed7690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7aec752-915b-49f1-85e7-e56ce47d7b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [time#123, temperature_2m#132, precipitation#129, rain#130, snowfall#131, weathercode#133]\n",
      "+- BatchScan[time#123, precipitation#129, rain#130, snowfall#131, temperature_2m#132, weathercode#133] Cassandra Scan: project_weather_data.weather_data_table\n",
      " - Cassandra Filters: []\n",
      " - Requested Columns: [time,precipitation,rain,snowfall,temperature_2m,weathercode]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " weather_3.createOrReplaceTempView(\"weather\")\n",
    "query =  '''\n",
    " SELECT time, temperature_2m, precipitation, rain, snowfall, weathercode FROM weather_data_table;\n",
    " '''\n",
    "spark.sql(query).explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a646de1a-fc4b-41b1-9b37-c64534335ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [time#123, temperature_2m#132, precipitation#129, rain#130, snowfall#131, weathercode#133]\n",
      "+- *(1) Filter (temperature_2m#132 > 60.0)\n",
      "   +- BatchScan[time#123, precipitation#129, rain#130, snowfall#131, temperature_2m#132, weathercode#133] Cassandra Scan: project_weather_data.weather_data_table\n",
      " - Cassandra Filters: []\n",
      " - Requested Columns: [time,precipitation,rain,snowfall,temperature_2m,weathercode]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query =  '''\n",
    " SELECT time, temperature_2m, precipitation, rain, snowfall, weathercode from weather_data_table  WHERE temperature_2m  > '60'\n",
    " '''\n",
    "spark.sql(query).explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa87df-59b6-4aa8-9e98-c60a7c711e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
